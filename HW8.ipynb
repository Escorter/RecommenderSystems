{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFzbjeIRVOi9"
   },
   "source": [
    "### Курсовой проект\n",
    "### Студент: Абрамов А.В."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T19:29:11.967649Z",
     "start_time": "2023-09-03T19:29:11.915594Z"
    },
    "id": "_SQOQwBnVaR_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit import als\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.metrics import precision_at_k, recall_at_k\n",
    "from src.utils import prefilter_items\n",
    "from src.recommenders import MainRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T19:29:14.775924Z",
     "start_time": "2023-09-03T19:29:13.283674Z"
    },
    "id": "UVVLgFk6Wk8m"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('retail_train.csv')\n",
    "item_features = pd.read_csv('product.csv')\n",
    "user_features = pd.read_csv('hh_demographic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:48:42.731063Z",
     "start_time": "2023-09-03T14:48:42.716077Z"
    },
    "id": "gOuBZCIoWuuq"
   },
   "outputs": [],
   "source": [
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:48:45.368795Z",
     "start_time": "2023-09-03T14:48:45.244132Z"
    },
    "id": "rpDNdG0dWu83"
   },
   "outputs": [],
   "source": [
    "val_lvl_1_size_weeks = 6\n",
    "val_lvl_2_size_weeks = 3\n",
    "\n",
    "data_train_lvl_1 = data[data['week_no'] < data['week_no'].max() - (val_lvl_1_size_weeks + val_lvl_2_size_weeks)]\n",
    "data_val_lvl_1 = data[(data['week_no'] >= data['week_no'].max() - (val_lvl_1_size_weeks + val_lvl_2_size_weeks)) &\n",
    "                      (data['week_no'] < data['week_no'].max() - (val_lvl_2_size_weeks))]\n",
    "\n",
    "data_train_lvl_2 = data_val_lvl_1.copy()\n",
    "data_val_lvl_2 = data[data['week_no'] >= data['week_no'].max() - val_lvl_2_size_weeks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:48:47.693012Z",
     "start_time": "2023-09-03T14:48:47.660128Z"
    },
    "id": "-ZIeokmeWvPe"
   },
   "outputs": [],
   "source": [
    "users_lvl_1 = data_train_lvl_1.user_id.unique()\n",
    "users_lvl_2 = data_val_lvl_1.user_id.unique()\n",
    "users_lvl_3 = data_val_lvl_2.user_id.unique()\n",
    "\n",
    "new_users_lvl_2 = list(set(users_lvl_2) - set(users_lvl_1))\n",
    "new_users_lvl_3 = list(set(users_lvl_3) - (set(users_lvl_1) | set(users_lvl_2)))\n",
    "\n",
    "add_to_lvl_2 = list(set(users_lvl_3) - (set(users_lvl_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:48:50.561039Z",
     "start_time": "2023-09-03T14:48:49.844955Z"
    },
    "id": "yIW2P8nCWvsm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\Рекомендательные системы\\Урок 8. Консультация к курсовому проекту\\src\\utils.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['price'] = data['sales_value'] / (np.maximum(data['quantity'], 1))\n"
     ]
    }
   ],
   "source": [
    "n_items_before = data_train_lvl_1['item_id'].nunique()\n",
    "data_train_lvl_1 = prefilter_items(data_train_lvl_1, item_features=item_features, take_n_popular=5000)\n",
    "n_items_after = data_train_lvl_1['item_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:48:53.660250Z",
     "start_time": "2023-09-03T14:48:53.636286Z"
    },
    "id": "MwhG6KMVWv7Y"
   },
   "outputs": [],
   "source": [
    "user_features['age_desc'].replace(\n",
    "    {'19-24': 22, '25-34': 30, '35-44': 40, '45-54': 50, '55-64': 60, '65+': 70},\n",
    "    inplace=True)\n",
    "\n",
    "user_features['marital_status_code'].replace(\n",
    "    {'U': 0, 'A': 1, 'B': 2}, inplace=True)\n",
    "\n",
    "user_features['income_desc'].replace(\n",
    "    {'Under 15K': 10, '15-24K': 20, '25-34K':30, '35-49K': 40,\n",
    "     '50-74K': 62, '75-99K': 87, '100-124K': 112, '125-149K': 137, \n",
    "     '150-174K': 162, '175-199K': 187, '200-249K': 225, '250K+':275}, inplace=True)\n",
    "\n",
    "user_features['homeowner_desc'].replace(\n",
    "    {'Unknown': 0, 'Probable Renter': 1, 'Renter': 2,\n",
    "     'Probable Owner': 3, 'Homeowner': 4}, inplace=True)\n",
    "\n",
    "user_features['hh_comp_desc'].replace(\n",
    "    {'Unknown': 0, 'Single Male': 1, 'Single Female': 2,\n",
    "     '1 Adult Kids': 3, '2 Adults No Kids': 4, '2 Adults Kids':5},inplace=True)\n",
    "\n",
    "user_features['household_size_desc'].replace({'5+': 5}, inplace=True) \n",
    "\n",
    "user_features['kid_category_desc'].replace(\n",
    "    {'None/Unknown': 0, '3+': 3}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:49:24.866955Z",
     "start_time": "2023-09-03T14:48:55.723922Z"
    },
    "id": "2U-kPZDVWwJv"
   },
   "outputs": [],
   "source": [
    "names = ['manufacturer', 'department', 'commodity_desc', 'sub_commodity_desc', 'curr_size_of_product']\n",
    "for name in names:\n",
    "    new_name = name + '_freq'\n",
    "    a = item_features[name].value_counts()\n",
    "    ind = a.index.tolist()\n",
    "    for i in ind:\n",
    "        item_features.loc[item_features[name] == i, new_name] = a[i]\n",
    "\n",
    "item_features['brand'] = np.where(item_features['brand']=='Private', 0, 1)\n",
    "\n",
    "commodities = item_features.commodity_desc.value_counts()\n",
    "commodities_list = commodities.keys().tolist()\n",
    "for i, name in enumerate(commodities_list):\n",
    "    item_features.loc[item_features['commodity_desc'] == name, 'commodity_category'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:49:45.126734Z",
     "start_time": "2023-09-03T14:49:45.111774Z"
    },
    "id": "pVLv1yfgXCD6"
   },
   "outputs": [],
   "source": [
    "def get_user_item_features(data_train_lvl_1):\n",
    "    X = data_train_lvl_1.copy()\n",
    "    X['hour'] = X['trans_time'] // 100\n",
    "    user_item_features = X.groupby(['user_id', 'item_id'])['hour'].median().reset_index()\n",
    "    user_item_features.columns = ['user_id', 'item_id', 'median_sales_hour']\n",
    "    \n",
    "    X['weekday'] = X['day'] % 7\n",
    "    df = X.groupby(['user_id', 'item_id'])['weekday'].median().reset_index()\n",
    "    df.columns = ['user_id', 'item_id', 'median_weekday']\n",
    "    user_item_features = user_item_features.merge(df, on=['user_id', 'item_id'])\n",
    "    \n",
    "    df = X.groupby('user_id')['day'].nunique().reset_index()\n",
    "    df['mean_visits_interval'] = (X.groupby('user_id')['day'].max() - X.groupby('user_id')['day'].min()) / df['day']\n",
    "    user_item_features = user_item_features.merge(df[['user_id', 'mean_visits_interval']], on=['user_id'])\n",
    "    \n",
    "    df = X.groupby(['user_id', 'basket_id'])['sales_value'].sum().reset_index()\n",
    "    df = df.groupby('user_id')['sales_value'].mean().reset_index()\n",
    "    df.columns = ['user_id', 'mean_check']\n",
    "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
    "    \n",
    "    df = X.groupby(['item_id'])['store_id'].nunique().reset_index()\n",
    "    df.columns = ['item_id', 'n_stores']\n",
    "    user_item_features = user_item_features.merge(df, on=['item_id'])\n",
    "    \n",
    "    df = X.groupby(['user_id'])['item_id'].nunique().reset_index()\n",
    "    df.columns = ['user_id', 'n_items']\n",
    "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
    "    \n",
    "    df = X.groupby(['user_id'])['item_id'].count().reset_index()\n",
    "    df.columns = ['user_id', 'n_transactions']\n",
    "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
    "    \n",
    "    df = X.groupby(['user_id', 'basket_id'])['item_id'].nunique().reset_index()\n",
    "    df1 = df.groupby('user_id')['item_id'].mean().reset_index()\n",
    "    df1.columns = ['user_id', 'mean_n_items_basket']\n",
    "    user_item_features = user_item_features.merge(df1, on=['user_id'])\n",
    "\n",
    "    df2 = df.groupby('user_id')['item_id'].max().reset_index()\n",
    "    df2.columns = ['user_id', 'max_n_items_basket']\n",
    "    user_item_features = user_item_features.merge(df2, on=['user_id'])\n",
    "\n",
    "    df3 = df.groupby('user_id')['item_id'].std().reset_index()\n",
    "    df3.columns = ['user_id', 'std_n_items_basket']\n",
    "    user_item_features = user_item_features.merge(df3, on=['user_id'])\n",
    "\n",
    "    recommender = MainRecommender(X)\n",
    "    df = recommender.model.item_factors\n",
    "    n_factors = recommender.model.factors\n",
    "    ind = list(recommender.id_to_itemid.values())\n",
    "    df = pd.DataFrame(df, index=ind).reset_index()\n",
    "    df.columns = ['item_id'] + ['factor_' + str(i + 1) for i in range(n_factors)]\n",
    "    user_item_features = user_item_features.merge(df, on=['item_id'])\n",
    "    \n",
    "    df = recommender.model.user_factors\n",
    "    ind = list(recommender.id_to_userid.values())\n",
    "    df = pd.DataFrame(df, index=ind).reset_index()\n",
    "    df.columns = ['user_id'] + ['user_factor_' + str(i + 1) for i in range(n_factors)]\n",
    "    user_item_features = user_item_features.merge(df, on=['user_id'])\n",
    "    \n",
    "    return user_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T14:49:50.395644Z",
     "start_time": "2023-09-03T14:49:47.324856Z"
    },
    "id": "U5NrbrNWXL0-"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'top_popular_n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_item_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_user_item_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_lvl_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mget_user_item_features\u001b[1;34m(data_train_lvl_1)\u001b[0m\n\u001b[0;32m     43\u001b[0m df3\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_n_items_basket\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m user_item_features \u001b[38;5;241m=\u001b[39m user_item_features\u001b[38;5;241m.\u001b[39mmerge(df3, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 46\u001b[0m recommender \u001b[38;5;241m=\u001b[39m \u001b[43mMainRecommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m df \u001b[38;5;241m=\u001b[39m recommender\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mitem_factors\n\u001b[0;32m     48\u001b[0m n_factors \u001b[38;5;241m=\u001b[39m recommender\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfactors\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'top_popular_n'"
     ]
    }
   ],
   "source": [
    "user_item_features = get_user_item_features(data_train_lvl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa5i84RXXODp"
   },
   "outputs": [],
   "source": [
    "def get_candidates(data_train_lvl_1, data_train_lvl_2, N, add_to_lvl_2):\n",
    "    recommender = MainRecommender(data_train_lvl_1)\n",
    "\n",
    "    users_lvl_1 = data_train_lvl_1['user_id'].unique()\n",
    "    users_lvl_2 = data_train_lvl_2['user_id'].unique().tolist()\n",
    "    if add_to_lvl_2:\n",
    "        users_lvl_2 += add_to_lvl_2\n",
    "\n",
    "    current_users = list(set(users_lvl_2) & set(users_lvl_1))    \n",
    "    new_users = list(set(users_lvl_2) - set(users_lvl_1))\n",
    "\n",
    "    df = pd.DataFrame(users_lvl_2, columns=['user_id'])\n",
    "    cond_1 = df['user_id'].isin(current_users)\n",
    "    df.loc[cond_1, 'candidates'] = df.loc[cond_1, 'user_id'].apply(\n",
    "        lambda x: recommender.get_own_recommendations(x, N))\n",
    "\n",
    "    if new_users:\n",
    "        cond_2 = df['user_id'].isin(new_users)\n",
    "        df.loc[cond_2, 'candidates'] = df.loc[cond_2, 'user_id'].apply(\n",
    "            lambda x: recommender.overall_top_purchases[:N])\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_targets_lvl_2(data_train_lvl_1, data_train_lvl_2, user_item_features, N, add_to_lvl_2=None):\n",
    "    \n",
    "    users_lvl_2 = get_candidates(data_train_lvl_1, data_train_lvl_2, N, add_to_lvl_2)\n",
    "    \n",
    "    df = pd.DataFrame({'user_id': users_lvl_2['user_id'].values.repeat(N),\n",
    "                       'item_id': np.concatenate(users_lvl_2['candidates'].values)})\n",
    "\n",
    "    targets_lvl_2 = data_train_lvl_2[['user_id', 'item_id']].copy()\n",
    "    targets_lvl_2['target'] = 1  \n",
    "\n",
    "    targets_lvl_2 = df.merge(targets_lvl_2, on=['user_id', 'item_id'], how='left')\n",
    "    targets_lvl_2['target'].fillna(0, inplace= True)\n",
    "    \n",
    "    targets_lvl_2 = targets_lvl_2.merge(\n",
    "        user_item_features, on=['user_id', 'item_id'], how='left')\n",
    "    \n",
    "    return targets_lvl_2\n",
    "N = 500\n",
    "targets_lvl_2 = get_targets_lvl_2(data_train_lvl_1, data_train_lvl_2, user_item_features, N, add_to_lvl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePeDrtOrXTTF"
   },
   "outputs": [],
   "source": [
    "SELECTED_FEATURES_NAMES = ['brand', 'manufacturer_freq', 'department_freq', 'commodity_desc_freq',\n",
    "                           'sub_commodity_desc_freq', 'curr_size_of_product_freq',\n",
    "                           'commodity_category', 'age_desc', 'marital_status_code', 'income_desc',\n",
    "                           'homeowner_desc', 'hh_comp_desc'\n",
    "                           \n",
    "                             \n",
    "                           'manufacturer',\n",
    "                           \n",
    "                           'median_sales_hour', 'median_weekday', #'mean_visits_interval',\n",
    "                           'mean_check', \n",
    "                           'n_stores', 'n_items', 'n_transactions', \n",
    "                           'mean_n_items_basket', 'max_n_items_basket', 'std_n_items_basket',\n",
    "                           'mean_n_item_categories_basket', 'max_n_item_categories_basket', \n",
    "                           'std_n_item_categories_basket',\n",
    "                           'factor_1', 'factor_2', 'factor_3', 'factor_4', 'factor_5',\n",
    "                           'factor_6', 'factor_7', 'factor_8', 'factor_9', 'factor_10',\n",
    "                           'factor_11', 'factor_12', 'factor_13', 'factor_14', 'factor_15',\n",
    "                           'factor_16', 'factor_17', 'factor_18', 'factor_19', 'factor_20',\n",
    "                           \n",
    "                           'user_factor_1', 'user_factor_2', 'user_factor_3', 'user_factor_4',\n",
    "                           'user_factor_5', 'user_factor_6', 'user_factor_7', 'user_factor_8',\n",
    "                           'user_factor_9', 'user_factor_10', 'user_factor_11', 'user_factor_12',\n",
    "                           'user_factor_13', 'user_factor_14', 'user_factor_15', 'user_factor_16',\n",
    "                           'user_factor_17', 'user_factor_18', 'user_factor_19', 'user_factor_20',\n",
    "                          ]\n",
    "categorical = ['marital_status_code','homeowner_desc', 'hh_comp_desc', 'manufacturer','commodity_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1UXK1juXWcr"
   },
   "outputs": [],
   "source": [
    "SELECTED_FEATURES_NAMES_cb = [i for i in SELECTED_FEATURES_NAMES if not i in categorical]\n",
    "def run_model_cb(targets_lvl_2):    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(targets_lvl_2[SELECTED_FEATURES_NAMES_cb].fillna(0),\n",
    "                                                          targets_lvl_2[['target']],\n",
    "                                                          test_size=0.2, random_state=16,\n",
    "                                                          stratify=targets_lvl_2[['target']])\n",
    "\n",
    "    dtrain = Pool(data=X_train, label=y_train)\n",
    "    dvalid = Pool(data=X_valid, label=y_valid) \n",
    "\n",
    "    params_cb = {\"n_estimators\":5000,\n",
    "                 \"loss_function\": \"Logloss\",\n",
    "                 \"eval_metric\": \"AUC\",\n",
    "                 \"task_type\": \"CPU\",\n",
    "                 \"max_bin\": 30,\n",
    "                 \"early_stopping_rounds\": 30,\n",
    "                 \"verbose\": 1000,\n",
    "                 \"l2_leaf_reg\": 80,\n",
    "                 \"thread_count\": 6,\n",
    "                 \"random_seed\": 51} \n",
    "\n",
    "    model_cb = CatBoostClassifier(**params_cb)\n",
    "    model_cb.fit(dtrain, eval_set=[dvalid])\n",
    "\n",
    "    \n",
    "    return model_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbaYTix9XdAx"
   },
   "outputs": [],
   "source": [
    "model_cb = run_model_cb(targets_lvl_2)\n",
    "\n",
    "print(model_cb.get_all_params())\n",
    "def run_model_lgb(targets_lvl_2):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(targets_lvl_2[SELECTED_FEATURES_NAMES].fillna(0),\n",
    "                                                          targets_lvl_2[['target']],\n",
    "                                                          test_size=0.2, random_state=16,\n",
    "                                                          stratify=targets_lvl_2[['target']])\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, y_train, categorical_feature=categorical)\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid, categorical_feature=categorical)\n",
    "\n",
    "    params_lgb = {\n",
    "                  \"objective\": \"binary\", \n",
    "                  \"metric\": \"auc\",\n",
    "                  \"num_boost_round\": 10000, \n",
    "                  \"n_jobs\": 8,\n",
    "                  \"force_row_wise\": True, \n",
    "                  \"seed\": 24} \n",
    "\n",
    "    model_lgb = lgb.train(params=params_lgb,\n",
    "                          train_set=dtrain,  \n",
    "                          valid_sets=[dtrain, dvalid],\n",
    "                          categorical_feature=categorical,\n",
    "                          verbose_eval=1000,\n",
    "                          early_stopping_rounds=30)\n",
    "    \n",
    "    return model_lgb\n",
    "model_lgb = run_model_lgb(targets_lvl_2)\n",
    "\n",
    "#[1322]\ttraining's auc: 0.954272\tvalid_1's auc: 0.912695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAy4Wly9XnS0"
   },
   "outputs": [],
   "source": [
    "predictions_lgb_train = model_lgb.predict(targets_lvl_2[SELECTED_FEATURES_NAMES].fillna(0))\n",
    "predictions_cb_train = model_cb.predict_proba(targets_lvl_2[SELECTED_FEATURES_NAMES_cb].fillna(0))[:, 1]\n",
    "\n",
    "preds_train = pd.DataFrame(zip(predictions_lgb_train, \n",
    "                               predictions_cb_train),\n",
    "                           columns=['lgb', 'cb']).mean(axis=1).values\n",
    "roc_auc_score(targets_lvl_2['target'], preds_train)\n",
    "\n",
    "def get_predictions(targets_lvl_2, raw_predictions, prefix='lgb'): \n",
    "    df = targets_lvl_2[['user_id', 'item_id']]\n",
    "    df['predictions'] = raw_predictions\n",
    "\n",
    "    df = df.groupby(['user_id', 'item_id'])['predictions'].median().reset_index()\n",
    "    df = df.sort_values(['predictions'], ascending=False).groupby(['user_id']).head(5)\n",
    "\n",
    "    df = df.groupby('user_id')['item_id'].unique().reset_index()\n",
    "    df.columns = ['user_id', prefix + '_recommendations']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_results(data_val_lvl_2, targets_lvl_2, preds_lgb, preds_cb, combined_preds):\n",
    "    result = data_val_lvl_2.groupby('user_id')['item_id'].unique().reset_index()\n",
    "    result.columns=['user_id', 'actual']\n",
    "    \n",
    "    prefixes = ['lgb', 'cb', 'cb_lgb']\n",
    "    predictions = [preds_lgb, preds_cb, combined_preds]\n",
    "    \n",
    "    for i, preds in enumerate(predictions):\n",
    "        df = get_predictions(targets_lvl_2, preds, prefixes[i])\n",
    "        result = result.merge(df, on='user_id', how='left')\n",
    "\n",
    "    return result\n",
    "result_lvl_2 = get_results(data_val_lvl_2, targets_lvl_2, \n",
    "                           predictions_lgb_train,\n",
    "                           predictions_cb_train, \n",
    "                           preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vR4SDh0X6Ai"
   },
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "result_lvl_2.apply(lambda row: precision_at_k(row['lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
    "# 0.29441723800195807\n",
    "\n",
    "#CatBoost\n",
    "result_lvl_2.apply(lambda row: precision_at_k(row['cb_recommendations'], row['actual'], 5), axis=1).mean()\n",
    "# 0.2863858961802145\n",
    "\n",
    "#Ensemble\n",
    "result_lvl_2.apply(lambda row: precision_at_k(row['cb_lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
    "# 0.30499510284035214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aX1n4_z5YFdn"
   },
   "outputs": [],
   "source": [
    "validation_weeks = 6\n",
    "data_train = data[data['week_no'] < data['week_no'].max() - validation_weeks]\n",
    "data_valid = data[data['week_no'] >= data['week_no'].max() - validation_weeks]\n",
    "test = pd.read_csv('predictions_basic.csv')\n",
    "users_lvl_1 = data_train.user_id.unique()\n",
    "users_lvl_2 = data_valid.user_id.unique()\n",
    "users_lvl_3 = test.user_id.unique()\n",
    "\n",
    "new_users_lvl_2 = list(set(users_lvl_2) - set(users_lvl_1))\n",
    "new_users_lvl_3 = list(set(users_lvl_3) - (set(users_lvl_1) | set(users_lvl_2)))\n",
    "\n",
    "add_to_lvl_2 = list(set(users_lvl_3) - (set(users_lvl_2)))\n",
    "\n",
    "new_users_lvl_2, new_users_lvl_3, len(add_to_lvl_2)\n",
    "\n",
    "from src.metrics import precision_at_k, recall_at_k\n",
    "from src.utils import prefilter_items\n",
    "from src.recommenders import MainRecommender\n",
    "import os\n",
    "\n",
    "os.chdir('src')\n",
    "%run utils.py\n",
    "n_items_before = data['item_id'].nunique()\n",
    "data_train = prefilter_items(data_train, item_features=item_features, take_n_popular=5000)\n",
    "n_items_after = data_train['item_id'].nunique()\n",
    "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))\n",
    "user_item_features = get_user_item_features(data_train)\n",
    "user_item_features.head(2)\n",
    "import os\n",
    "os.chdir('src')\n",
    "%run recommenders.py\n",
    "targets_test = get_targets_lvl_2(data_train, data_valid, user_item_features, N, add_to_lvl_2)\n",
    "\n",
    "print(f'число пользователей: {targets_test.user_id.nunique()}')\n",
    "print(f'среднее число покупок: {round(targets_test[\"target\"].mean(), 4)}')\n",
    "\n",
    "targets_test.head(2)\n",
    "SELECTED_FEATURES_NAMES = ['median_sales_hour', 'median_weekday', \n",
    "                           'mean_check', \n",
    "                           'n_stores', 'n_items', 'n_transactions', \n",
    "                           'mean_n_items_basket', 'max_n_items_basket', \n",
    "                           \n",
    "                          \n",
    "                           'factor_1', 'factor_2', 'factor_3', 'factor_4', 'factor_5',\n",
    "                           'factor_6', 'factor_7', 'factor_8', 'factor_9', 'factor_10',\n",
    "                           'factor_11', 'factor_12', 'factor_13', 'factor_14', 'factor_15',\n",
    "                           'factor_16', 'factor_17', 'factor_18', 'factor_19', 'factor_20',\n",
    "                           \n",
    "                           'user_factor_1', 'user_factor_2', 'user_factor_3', 'user_factor_4',\n",
    "                           'user_factor_5', 'user_factor_6', 'user_factor_7', 'user_factor_8',\n",
    "                           'user_factor_9', 'user_factor_10', 'user_factor_11', 'user_factor_12',\n",
    "                           'user_factor_13', 'user_factor_14', 'user_factor_15', 'user_factor_16',\n",
    "                           'user_factor_17', 'user_factor_18', 'user_factor_19', 'user_factor_20',\n",
    "                          ]\n",
    "categorical = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KltiIQ6WYS6h"
   },
   "outputs": [],
   "source": [
    "model_lgb = run_model_lgb(targets_test)\n",
    "# [1558]\ttraining's auc: 0.900737\tvalid_1's auc: 0.87275\n",
    "SELECTED_FEATURES_NAMES_cb = [i for i in SELECTED_FEATURES_NAMES if not i in categorical]\n",
    "model_cb = run_model_cb(targets_test)\n",
    "# bestTest = 0.8620786343\n",
    "# bestIteration = 3899\n",
    "predictions_lgb_test = model_lgb.predict(targets_test[SELECTED_FEATURES_NAMES].fillna(0))\n",
    "predictions_cb_test = model_cb.predict_proba(targets_test[SELECTED_FEATURES_NAMES_cb].fillna(0))[:, 1]\n",
    "\n",
    "preds_test = pd.DataFrame(zip(predictions_lgb_test, predictions_cb_test),columns=['lgb', 'cb']).mean(axis=1).values\n",
    "roc_auc_score(targets_test['target'], preds_test)\n",
    "# 0.8887098749469134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1mtzaU6YZYz"
   },
   "outputs": [],
   "source": [
    "def get_results_1(data_val_lvl_2, targets_lvl_2, preds_lgb, preds_cb, combined_preds):\n",
    "    result = data_val_lvl_2.groupby('user_id')['actual'].unique().reset_index()\n",
    "    result.columns=['user_id', 'actual']\n",
    "    \n",
    "    prefixes = ['lgb', 'cb', 'cb_lgb']\n",
    "    predictions = [preds_lgb, preds_cb, combined_preds]\n",
    "    \n",
    "    for i, preds in enumerate(predictions):\n",
    "        df = get_predictions(targets_lvl_2, preds, prefixes[i])\n",
    "        result = result.merge(df, on='user_id', how='left')\n",
    "\n",
    "    return result\n",
    "result_test = get_results_1(test, targets_test, \n",
    "                           predictions_lgb_test,\n",
    "                           predictions_cb_test, \n",
    "                           preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCtIt6IiYcJL"
   },
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "result_test.apply(lambda row: precision_at_k(row['lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
    "# 0.3131034482758621\n",
    "\n",
    "#CatBooost\n",
    "result_test.apply(lambda row: precision_at_k(row['cb_recommendations'], row['actual'], 5), axis=1).mean()\n",
    "# 0.33389920424403186\n",
    "\n",
    "#Ensemble\n",
    "result_test.apply(lambda row: precision_at_k(row['cb_lgb_recommendations'], row['actual'], 5), axis=1).mean()\n",
    "# 0.3335809018567639\n",
    "Сохранение результатов\n",
    "\n",
    "df = result_test[['user_id', 'cb_lgb_recommendations']].copy()\n",
    "df.to_csv('predictions_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6gYDgPeN5WPC",
    "Yrxa6_7jjSpw",
    "CKlUurWBExd8",
    "Djo8QWzY_mUb",
    "Kt_wmIdcgOfB",
    "BMeCB4Vc6yqc"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
